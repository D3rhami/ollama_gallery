{
  "description": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
  "capabilities": [],
  "parameters": [
    "16b",
    "236b"
  ],
  "pull_str": "1.1M",
  "pull": 1100000,
  "tag_count": 60,
  "updated": "1 year ago",
  "update_date": "2024-09-16",
  "link": "https://ollama.com/library/deepseek-coder-v2",
  "popularity": 42
}