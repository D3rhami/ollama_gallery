{
  "description": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
  "capabilities": [],
  "parameters": [
    "1.1b"
  ],
  "pull_str": "2.9M",
  "pull": 2900000,
  "tag_count": "36",
  "updated": "1 year ago",
  "update_date": "2024-10-02",
  "link": "https://ollama.com/library/tinyllama",
  "popularity": 26
}