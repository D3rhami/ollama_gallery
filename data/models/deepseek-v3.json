{
  "description": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
  "capabilities": [],
  "parameters": [
    "671b"
  ],
  "pull_str": "3.1M",
  "pull": 3100000,
  "tag_count": 3,
  "updated": "12 months ago",
  "update_date": "2025-01-09",
  "link": "https://ollama.com/library/deepseek-v3",
  "popularity": 37
}