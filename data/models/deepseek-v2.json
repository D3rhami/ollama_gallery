{
  "description": "A strong, economical, and efficient Mixture-of-Experts language model.",
  "capabilities": [],
  "parameters": [
    "16b",
    "236b"
  ],
  "pull_str": "489.1K",
  "pull": 489100,
  "tag_count": "34",
  "updated": "1 year ago",
  "update_date": "2025-02-27",
  "link": "https://ollama.com/library/deepseek-v2",
  "popularity": 93
}