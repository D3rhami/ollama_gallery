{
  "description": "ðŸŒ‹ LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
  "capabilities": [
    "vision"
  ],
  "parameters": [
    "7b",
    "13b",
    "34b"
  ],
  "pull_str": "10.2M",
  "pull": 10200000,
  "tag_count": "98",
  "updated": "1 year ago",
  "update_date": "2024-10-05",
  "link": "https://ollama.com/library/llava",
  "popularity": 14
}