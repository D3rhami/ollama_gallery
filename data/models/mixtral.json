{
  "description": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
  "capabilities": [
    "tools"
  ],
  "parameters": [
    "8x7b",
    "8x22b"
  ],
  "pull_str": "1.4M",
  "pull": 1400000,
  "tag_count": "70",
  "updated": "9 months ago",
  "update_date": "2024-12-21",
  "link": "https://ollama.com/library/mixtral",
  "popularity": 38
}