{
  "description": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.",
  "capabilities": [],
  "parameters": [
    "8x7b",
    "8x22b"
  ],
  "pull_str": "990.6K",
  "pull": 990600,
  "tag_count": 60,
  "updated": "1 year ago",
  "update_date": "2025-02-11",
  "link": "https://ollama.com/library/dolphin-mixtral",
  "popularity": 58
}